{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-World Data Wrangling Examples\n",
    "\n",
    "This tutorial demonstrates practical applications of data-wrangler across different domains and use cases. We'll explore how to handle messy, real-world data scenarios effectively.\n",
    "\n",
    "## Scenarios Covered\n",
    "\n",
    "1. **Customer Feedback Analysis**: Processing mixed review data\n",
    "2. **Research Data Integration**: Combining surveys, papers, and datasets\n",
    "3. **Content Recommendation**: Building similarity systems\n",
    "4. **Data Pipeline Automation**: Using decorators for robust preprocessing\n",
    "\n",
    "Let's dive into practical examples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import datawrangler as dw\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datawrangler import funnel\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Customer Feedback Analysis\n",
    "\n",
    "Imagine you're analyzing customer feedback from multiple sources: surveys, social media, emails, and review sites. Each source has different formats and structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Simulate different types of customer feedback\n",
    "survey_responses = [\n",
    "    \"The product quality exceeded my expectations. Highly recommend!\",\n",
    "    \"Delivery was slow but customer service was helpful.\",\n",
    "    \"Great value for money. Will purchase again.\",\n",
    "    \"Product broke after one week. Very disappointed.\",\n",
    "    \"Easy to use interface. Love the new features.\"\n",
    "]\n",
    "\n",
    "social_media_posts = [\n",
    "    \"Just tried @YourBrand and wow! \ud83d\udd25 #amazing #quality\",\n",
    "    \"Not impressed with recent purchase from @YourBrand \ud83d\ude1e\",\n",
    "    \"Customer support team went above and beyond! \ud83d\udc4f #excellent\",\n",
    "    \"Website was confusing, took forever to find what I needed\",\n",
    "    \"Fast shipping and great packaging! @YourBrand #satisfied\"\n",
    "]\n",
    "\n",
    "email_feedback = [\n",
    "    \"I wanted to express my satisfaction with your recent service...\",\n",
    "    \"The invoice process needs improvement. Too complicated.\",\n",
    "    \"Your technical documentation is excellent and comprehensive.\",\n",
    "    \"Had trouble with the mobile app. Crashes frequently.\",\n",
    "    \"Pricing is competitive compared to other options.\"\n",
    "]\n",
    "\n",
    "print(f\"Survey responses: {len(survey_responses)}\")\n",
    "print(f\"Social media posts: {len(social_media_posts)}\")\n",
    "print(f\"Email feedback: {len(email_feedback)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unified Analysis with Data Wrangler\n",
    "\n",
    "Now let's use data-wrangler to analyze all feedback sources together, using modern sentence-transformers for semantic understanding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "@funnel\n",
    "def analyze_customer_sentiment(feedback_data, text_kwargs={'model': 'all-mpnet-base-v2'}):\n",
    "    \"\"\"Comprehensive customer feedback analysis\"\"\"\n",
    "    \n",
    "    print(f\"Analyzing {feedback_data.shape[0]} feedback items...\")\n",
    "    print(f\"Embedding dimensions: {feedback_data.shape[1]}\")\n",
    "    \n",
    "    # Cluster feedback into themes\n",
    "    n_clusters = 3\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    clusters = kmeans.fit_predict(feedback_data)\n",
    "    \n",
    "    # Calculate sentiment indicators (simple proxy using embedding statistics)\n",
    "    sentiment_scores = feedback_data.mean(axis=1)  # Simple aggregation\n",
    "    \n",
    "    results = pd.DataFrame({\n",
    "        'cluster': clusters,\n",
    "        'sentiment_proxy': sentiment_scores\n",
    "    })\n",
    "    \n",
    "    # Summary statistics\n",
    "    summary = {\n",
    "        'total_feedback': len(feedback_data),\n",
    "        'themes_identified': n_clusters,\n",
    "        'avg_sentiment': sentiment_scores.mean(),\n",
    "        'sentiment_std': sentiment_scores.std(),\n",
    "        'cluster_distribution': results['cluster'].value_counts().to_dict()\n",
    "    }\n",
    "    \n",
    "    return results, summary\n",
    "\n",
    "# Combine all feedback sources\n",
    "all_feedback = survey_responses + social_media_posts + email_feedback\n",
    "\n",
    "# Analyze with one function call\n",
    "feedback_analysis, summary = analyze_customer_sentiment(all_feedback)\n",
    "\n",
    "print(\"\\n=== Customer Feedback Analysis Summary ===\")\n",
    "for key, value in summary.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Feedback Themes\n",
    "\n",
    "Let's create a visualization to understand the feedback distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create source labels for visualization\n",
    "source_labels = (['Survey'] * len(survey_responses) + \n",
    "                ['Social Media'] * len(social_media_posts) + \n",
    "                ['Email'] * len(email_feedback))\n",
    "\n",
    "feedback_analysis['source'] = source_labels\n",
    "feedback_analysis['feedback_text'] = all_feedback\n",
    "\n",
    "# Plot cluster distribution by source\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "cluster_source_counts = feedback_analysis.groupby(['cluster', 'source']).size().unstack(fill_value=0)\n",
    "cluster_source_counts.plot(kind='bar', ax=plt.gca())\n",
    "plt.title('Feedback Themes by Source')\n",
    "plt.xlabel('Theme Cluster')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Source')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(feedback_analysis['sentiment_proxy'], feedback_analysis['cluster'], \n",
    "           c=feedback_analysis['cluster'], cmap='viridis', alpha=0.7)\n",
    "plt.xlabel('Sentiment Proxy Score')\n",
    "plt.ylabel('Theme Cluster')\n",
    "plt.title('Sentiment vs Theme Distribution')\n",
    "plt.colorbar(label='Cluster')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show example feedback from each cluster\n",
    "print(\"\\n=== Example Feedback by Theme ===\")\n",
    "for cluster in sorted(feedback_analysis['cluster'].unique()):\n",
    "    cluster_examples = feedback_analysis[feedback_analysis['cluster'] == cluster]\n",
    "    print(f\"\\nCluster {cluster} Examples:\")\n",
    "    for idx, row in cluster_examples.head(2).iterrows():\n",
    "        print(f\"  - ({row['source']}) {row['feedback_text'][:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Example 2: Research Data Integration\n\nAcademic researchers often need to combine data from multiple sources: research papers, survey responses, experimental datasets, and literature reviews. Let's see how data-wrangler can streamline this process.\"\n  }\n  }\n ],\n \"metadata\": {",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Simulate research data from different sources\npaper_abstracts = [\n    \"Deep learning models have shown remarkable performance in natural language processing tasks.\",\n    \"Transformer architectures revolutionized machine translation and text understanding.\",\n    \"BERT and its variants achieved state-of-the-art results across multiple NLP benchmarks.\",\n    \"Large language models demonstrate emergent capabilities in few-shot learning scenarios.\",\n    \"Fine-tuning pretrained models enables efficient adaptation to specific domains.\"\n]\n\nsurvey_responses = [\n    \"I find AI tools helpful for research but worry about accuracy.\",\n    \"Machine learning has accelerated my data analysis workflow significantly.\", \n    \"The interpretability of AI models is crucial for scientific applications.\",\n    \"Collaborative AI tools enhance productivity in research teams.\",\n    \"Ethical considerations in AI research are becoming increasingly important.\"\n]\n\nexperimental_notes = [\n    \"Model A achieved 94% accuracy on validation set with minimal overfitting.\",\n    \"Hyperparameter tuning improved F1-score from 0.85 to 0.91.\",\n    \"Data augmentation techniques reduced training variance significantly.\",\n    \"Cross-validation results show consistent performance across folds.\",\n    \"Ensemble methods provided 3% improvement over single models.\"\n]\n\nprint(f\"Research data sources:\")\nprint(f\"- Paper abstracts: {len(paper_abstracts)}\")\nprint(f\"- Survey responses: {len(survey_responses)}\")\nprint(f\"- Experimental notes: {len(experimental_notes)}\")",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}